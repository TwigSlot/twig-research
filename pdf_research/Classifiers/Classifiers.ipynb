{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "\n",
    "import sklearn.preprocessing as preproc\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, plot_confusion_matrix, ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense, Dropout, MaxPooling1D\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import layers\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from gc import callbacks\n",
    "\n",
    "import pickle\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feaure engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy data to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\j\\AppData\\Local\\Temp\\ipykernel_12072\\3055522815.py:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train_vec = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
      "C:\\Users\\j\\AppData\\Local\\Temp\\ipykernel_12072\\3055522815.py:40: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_test_vec = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('./train.csv')\n",
    "test_df = pd.read_csv('./test.csv')\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Text preprocessing/cleaning function'''\n",
    "    \n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove symbols, special characters\n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Tokenize \n",
    "    text =  nltk.WordPunctTokenizer().tokenize(text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "train_df['Cleaned'] = list(map(clean_text, train_df.text))\n",
    "test_df['Cleaned'] = list(map(clean_text, test_df.text))\n",
    "\n",
    "X_train, y_train = train_df['Cleaned'], train_df['label']\n",
    "X_test, y_test = test_df['Cleaned'], test_df['label']\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                   vector_size=256,\n",
    "                                   window=4,\n",
    "                                   min_count=1)\n",
    "\n",
    "words = set(w2v_model.wv.index_to_key)\n",
    "\n",
    "X_train_vec = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_train])\n",
    "\n",
    "X_test_vec = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_test])\n",
    "\n",
    "X_train = []\n",
    "for v in X_train_vec:\n",
    "    if v.size:\n",
    "        X_train.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_train.append(np.zeros(256, dtype=float))\n",
    "  \n",
    "X_test = []\n",
    "for v in X_test_vec:\n",
    "    if v.size:\n",
    "        X_test.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_test.append(np.zeros(256, dtype=float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\j/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwasabee\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\j\\Desktop\\qwe\\wandb\\run-20220929_194417-3399r1yn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wasabee/Title%20classifier/runs/3399r1yn\" target=\"_blank\">rose-meadow-1</a></strong> to <a href=\"https://wandb.ai/wasabee/Title%20classifier\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/wasabee/Title%20classifier/runs/3399r1yn?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x233f0525670>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logger\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters {'C': 100, 'solver': 'lbfgs'}\n",
      "Best score 0.8933666961044059\n",
      "Accuracy Score: 0.6666666666666666\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.25      0.40         4\n",
      "           1       0.62      1.00      0.77         5\n",
      "\n",
      "    accuracy                           0.67         9\n",
      "   macro avg       0.81      0.62      0.58         9\n",
      "weighted avg       0.79      0.67      0.61         9\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\j\\Desktop\\qwe\\wandb\\run-20220929_204919-3v0jz9fe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wasabee/Title%20classifier/runs/3v0jz9fe\" target=\"_blank\">wobbly-paper-1</a></strong> to <a href=\"https://wandb.ai/wasabee/Title%20classifier\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Plotting Logistic regression.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m could not find any of attributes feature_importances_, feature_log_prob_, coef_ on classifier. Cannot plot feature importances.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged feature importances.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged confusion matrix.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged summary metrics.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged class proportions.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged calibration curve.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged roc curve.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged precision-recall curve.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">wobbly-paper-1</strong>: <a href=\"https://wandb.ai/wasabee/Title%20classifier/runs/3v0jz9fe\" target=\"_blank\">https://wandb.ai/wasabee/Title%20classifier/runs/3v0jz9fe</a><br/>Synced 6 W&B file(s), 6 media file(s), 6 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220929_204919-3v0jz9fe\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels=['0', '1']\n",
    "\n",
    "# Logistic regression \n",
    "\n",
    "param_log = {'solver':['lbfgs'],\n",
    "              'C':[1, 5, 10, 50, 100]}\n",
    "\n",
    "# Create model\n",
    "logr_clf = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# 10 fold cv hyper-parameters tuning\n",
    "clf_log = GridSearchCV(logr_clf,\n",
    "                       param_grid=param_log, \n",
    "                       cv=10, \n",
    "                       scoring='accuracy',\n",
    "                       refit=True) \n",
    "\n",
    "# Fit\n",
    "clf_log.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_log.best_params_)\n",
    "print(\"Best score\", clf_log.best_score_)\n",
    "\n",
    "# Predict on test\n",
    "logr_y_pred = clf_log.predict(X_test)\n",
    "logr_y_proba = clf_log.predict_proba(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test,logr_y_pred)))\n",
    "print('Classification report\\n', classification_report(y_test, logr_y_pred)) \n",
    "\n",
    "wandb.init(project=\"Title classifier\", reinit=True)\n",
    "wandb.run.name = 'Logistic regression'\n",
    "wandb.sklearn.plot_classifier(clf_log, X_train, X_test, y_train, y_test, logr_y_pred, logr_y_proba, labels,\n",
    "                                                         model_name='Logistic regression', feature_names=None)\n",
    "wandb.run.save\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters {'C': 10, 'gamma': 5, 'kernel': 'rbf'}\n",
      "Best score 0.9267272370451647\n",
      "Accuracy Score: 0.7777777777777778\n",
      "Classification report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         4\n",
      "           1       0.71      1.00      0.83         5\n",
      "\n",
      "    accuracy                           0.78         9\n",
      "   macro avg       0.86      0.75      0.75         9\n",
      "weighted avg       0.84      0.78      0.76         9\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\j\\Desktop\\qwe\\wandb\\run-20220929_205601-2dvh0zdl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wasabee/Title%20classifier/runs/2dvh0zdl\" target=\"_blank\">vivid-brook-2</a></strong> to <a href=\"https://wandb.ai/wasabee/Title%20classifier\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Plotting Support vector.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m could not find any of attributes feature_importances_, feature_log_prob_, coef_ on classifier. Cannot plot feature importances.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged feature importances.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged confusion matrix.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged summary metrics.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged class proportions.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged calibration curve.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged roc curve.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged precision-recall curve.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">vivid-brook-2</strong>: <a href=\"https://wandb.ai/wasabee/Title%20classifier/runs/2dvh0zdl\" target=\"_blank\">https://wandb.ai/wasabee/Title%20classifier/runs/2dvh0zdl</a><br/>Synced 6 W&B file(s), 6 media file(s), 6 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220929_205601-2dvh0zdl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Svc\n",
    "\n",
    "param_svc = {'kernel':['rbf'],\n",
    "              'C':[0.1, 1, 5, 10],\n",
    "              'gamma': [0.01, 0.1, 1, 5]}\n",
    "\n",
    "# Create model\n",
    "svc_clf = SVC(probability=True)\n",
    "\n",
    "# 10 fold cv hyper-parameters tuning\n",
    "clf_svc = GridSearchCV(svc_clf,\n",
    "                       param_grid=param_svc, \n",
    "                       cv=10, \n",
    "                       scoring='accuracy',\n",
    "                       refit=True) \n",
    "# Fit\n",
    "clf_svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_svc.best_params_)\n",
    "print(\"Best score\", clf_svc.best_score_)\n",
    "\n",
    "# Predict on test\n",
    "svc_y_pred = clf_svc.predict(X_test)\n",
    "svc_y_proba = clf_svc.predict_proba(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test,svc_y_pred)))\n",
    "print('Classification report\\n', classification_report(y_test, svc_y_pred)) \n",
    "\n",
    "wandb.init(project=\"Title classifier\", reinit=True)\n",
    "wandb.run.name = 'Support vector'\n",
    "wandb.sklearn.plot_classifier(clf_svc, X_train, X_test, y_train, y_test, svc_y_pred, svc_y_proba, labels,\n",
    "                                                         model_name='Support vector', feature_names=None)\n",
    "wandb.run.save\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Forest\n",
    "\n",
    "param_rf = { \n",
    "    'n_estimators': [5, 10, 20, 50, 100, 150],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth' : [1,2,3,4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=41)\n",
    "\n",
    "clf_rf = GridSearchCV(estimator=rf_clf, \n",
    "                      param_grid=param_rf, \n",
    "                      scoring='accuracy',\n",
    "                      cv=10,\n",
    "                      refit=True)\n",
    "\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_rf.best_params_)\n",
    "print(\"Best score\", clf_rf.best_score_)\n",
    "\n",
    "rfc_y_pred = clf_rf.predict(X_test)\n",
    "rfc_y_proba = clf_rf.predict_proba(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test, rfc_y_pred)))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, rfc_y_pred))\n",
    "\n",
    "wandb.init(project=\"Title classifier\", reinit=True)\n",
    "wandb.run.name = 'Random forest'\n",
    "wandb.sklearn.plot_classifier(clf_rf, X_train, X_test, y_train, y_test, rfc_y_pred, rfc_y_proba, labels\n",
    "                                                         model_name='Random forest', feature_names=None)\n",
    "wandb.run.save\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decision tree\n",
    "\n",
    "param_dt = {'max_features': ['sqrt', 'log2'],\n",
    "              'ccp_alpha': [0.5, 0.1, .01, .001],\n",
    "              'max_depth' : [1,2,3,4,5,6,7,8,9],\n",
    "              'criterion' :['gini', 'entropy']\n",
    "             }\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "clf_dt = GridSearchCV(dt_clf, \n",
    "                      param_grid=param_dt, \n",
    "                      scoring='accuracy',\n",
    "                      cv=10,\n",
    "                      refit=True)\n",
    "\n",
    "clf_dt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_dt.best_params_)\n",
    "print(\"Best score\", clf_dt.best_score_)\n",
    "\n",
    "dt_y_pred = clf_dt.predict(X_test)\n",
    "dt_y_proba = clf_dt.predict_proba(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test, dt_y_pred)))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, dt_y_pred))\n",
    "\n",
    "wandb.init(project=\"Title classifier\", reinit=True)\n",
    "wandb.run.name = 'Decision tree'\n",
    "wandb.sklearn.plot_classifier(clf_dt, X_train, X_test, y_train, y_test, dt_y_pred, dt_y_proba, labels,\n",
    "                                                         model_name='Decision tree', feature_names=None)\n",
    "wandb.run.save\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "\n",
    "param_knn = { \n",
    "    'n_neighbors': list(range(1,11)),\n",
    "}\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "clf_knn = GridSearchCV(estimator=rf_clf, \n",
    "                      param_grid=param_rf, \n",
    "                      scoring='accuracy',\n",
    "                      cv=10,\n",
    "                      refit=True)\n",
    "\n",
    "clf_knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_knn.best_params_)\n",
    "print(\"Best score\", clf_knn.best_score_)\n",
    "\n",
    "knn_y_pred = clf_knn.predict(X_test)\n",
    "knn_y_proba = clf_knn.predict_proba(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test, knn_y_pred)))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, knn_y_pred))\n",
    "\n",
    "wandb.init(project=\"Title classifier\", reinit=True)\n",
    "wandb.run.name = 'KNN'\n",
    "wandb.sklearn.plot_classifier(clf_knn, X_train, X_test, y_train, y_test, knn_y_pred, knn_y_proba, labels,\n",
    "                                                         model_name='KNN', feature_names=None)\n",
    "wandb.run.save\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB\n",
    "\n",
    "param_xgb = {\n",
    "        'n_estimators': range(20, 400, 20),\n",
    "        'learning_rate': [1e-1, 1e-2, 5e-3, 5e-4],\n",
    "        'min_child_weight': [1, 3, 5, 7, 9],\n",
    "        'gamma': [0.5, 1, 1.5, 2],\n",
    "        'subsample': [0.5, 0.75, 1.0],\n",
    "        'colsample_bytree': [0.5, 0.75, 1.0],\n",
    "        'max_depth': [2, 3, 4, 5, 6, 7, 8]\n",
    "        }\n",
    "\n",
    "xgb_clf = XGBClassifier(objective='binary:logistic', seed=41)\n",
    "\n",
    "clf_xgb = GridSearchCV(estimator=xgb_clf, \n",
    "                      param_grid=param_xgb, \n",
    "                      scoring='accuracy',\n",
    "                      cv=10,\n",
    "                      refit=True)\n",
    "\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_xgb.best_params_)\n",
    "print(\"Best score\", clf_xgb.best_score_)\n",
    "\n",
    "xgb_y_pred = clf_xgb.predict(X_test)\n",
    "xgb_y_proba = clf_xgb.predict_proba(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test, xgb_y_pred)))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, xgb_y_pred))\n",
    "\n",
    "wandb.init(project=\"Title classifier\", reinit=True)\n",
    "wandb.run.name = 'XGB'\n",
    "wandb.sklearn.plot_classifier(clf_xgb, X_train, X_test, y_train, y_test, xgb_y_pred, xgb_y_proba, labels,\n",
    "                                                         model_name='XGB', feature_names=None)\n",
    "wandb.run.save\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_len = 512\n",
    "max_tokens = 300\n",
    "max_words = 100\n",
    "n_classes = 2\n",
    "\n",
    "sweep_config = {\n",
    "   'method': 'grid',\n",
    "   \n",
    "   'parameters': {\n",
    "       \n",
    "       'neurons': {\n",
    "           'values': [32, 64, 128]\n",
    "       },\n",
    "       \n",
    "       'f': {\n",
    "           'values': [64, 128, 256]\n",
    "       },\n",
    "       \n",
    "       'bs': {\n",
    "           'values': [8, 16, 32, 64]\n",
    "       }\n",
    "   }\n",
    "}\n",
    "\n",
    "wandb.init(project=\"Title classifier\", reinit=True)\n",
    "wandb.run.name = 'CNN model tuning'\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "       \n",
    "   configs = {\n",
    "       'neurons': 32,\n",
    "       'f': 64,\n",
    "       'bs': 8\n",
    "   }\n",
    "   \n",
    "   # Specify the other hyperparameters to the configuration\n",
    "   config = wandb.config\n",
    "   config.epochs = 50\n",
    "   \n",
    "   model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(max_tokens, )),\n",
    "            \n",
    "            layers.Embedding(input_dim=max_words, \n",
    "                             output_dim=embed_len,  \n",
    "                             input_length=max_tokens),\n",
    "            \n",
    "            # Block 1\n",
    "            layers.Conv1D(wandb.config.f, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling1D(pool_size=(3, 3)),\n",
    "            layers.Dropout(0.3),                  \n",
    "            \n",
    "            # Block 2\n",
    "            layers.Conv1D(wandb.config.f, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling1D(pool_size=(3, 3)),\n",
    "            layers.Dropout(0.3),                \n",
    "            \n",
    "            # FC 1\n",
    "            layers.Flatten(),                                     \n",
    "            layers.Dense(wandb.config.neurons, activation=\"relu\"),             \n",
    "            \n",
    "            # Output \n",
    "            layers.Dense(n_classes, activation=\"softmax\")     \n",
    "        ]\n",
    "    )\n",
    "\n",
    "   model.compile(optimizer='adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "   \n",
    "   model.fit(X_train, y_train, epochs=config.epochs, batch_size=config.bs,\n",
    "             validation_split=0.1, callbacks=[WandbCallback()])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.save\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create model\n",
    "\n",
    "# embed_len = 512\n",
    "# max_tokens = 300\n",
    "# max_words = 100\n",
    "# n_classes = 2\n",
    "\n",
    "# def create_model(f, neuron):\n",
    "    \n",
    "#     model = keras.Sequential(\n",
    "#         [\n",
    "#             keras.Input(shape=(max_tokens, )),\n",
    "            \n",
    "#             layers.Embedding(input_dim=max_words, \n",
    "#                              output_dim=embed_len,  \n",
    "#                              input_length=max_tokens),\n",
    "            \n",
    "#             # Block 1\n",
    "#             layers.Conv1D(f, kernel_size=(3, 3), activation=\"relu\"),\n",
    "#             layers.MaxPooling1D(pool_size=(3, 3)),\n",
    "#             layers.Dropout(0.3),                  \n",
    "            \n",
    "#             # Block 2\n",
    "#             layers.Conv1D(f, kernel_size=(3, 3), activation=\"relu\"),\n",
    "#             layers.MaxPooling1D(pool_size=(3, 3)),\n",
    "#             layers.Dropout(0.3),                \n",
    "            \n",
    "#             # FC 1\n",
    "#             layers.Flatten(),                                     \n",
    "#             layers.Dense(neuron, activation=\"relu\"),             \n",
    "            \n",
    "#             # Output \n",
    "#             layers.Dense(n_classes, activation=\"softmax\")     \n",
    "#         ]\n",
    "#     )\n",
    "    \n",
    "#     model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KerasClassifier(build_fn=create_model,\n",
    "#                         f=[128,256],\n",
    "#                         epochs=[50],\n",
    "#                         neuron=[32,64],\n",
    "#                         batch_size=[32,64],)\n",
    "\n",
    "# # Model params\n",
    "# params={'batch_size':[32,64], \n",
    "#         'epochs':[50],\n",
    "#         'neuron':[32,64],\n",
    "#         'f':[128,256],\n",
    "#         }\n",
    "\n",
    "# # Grid search cv\n",
    "# gs = GridSearchCV(estimator=model, \n",
    "#                   param_grid=params, \n",
    "#                   cv=10,\n",
    "#                   refit=True)\n",
    "# # Fit\n",
    "# gs = gs.fit(X_train, y_train)\n",
    "\n",
    "# print('Best acc:', gs.best_score_)\n",
    "# print('Best params:', gs.best_params_)\n",
    "\n",
    "# mean = gs.cv_results_['mean_test_score']\n",
    "# stds = gs.cv_results_['std_test_score']\n",
    "# params = gs.cv_results_['params']\n",
    "\n",
    "# for mean, stdev, param in zip(mean, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain with best parameters\n",
    "\n",
    "wandb.init(project=\"Title classifier\", reinit=True)\n",
    "wandb.run.name = 'Best params CNN model training'\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(max_tokens, )),\n",
    "        \n",
    "        layers.Embedding(input_dim=max_words, \n",
    "                    output_dim=embed_len,  \n",
    "                    input_length=max_tokens),\n",
    "        \n",
    "        # Block 1\n",
    "        layers.Conv1D(256, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling1D(pool_size=(3, 3)),                  \n",
    "        layers.Dropout(0.3),                  \n",
    "\n",
    "        # Block 2\n",
    "        layers.Conv1D(256, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling1D(pool_size=(3, 3)),                \n",
    "        layers.Dropout(0.3),                  \n",
    "\n",
    "        # FC 1\n",
    "        layers.Flatten(),                                     \n",
    "        layers.Dense(64, activation=\"relu\"),                \n",
    "        \n",
    "        # Output \n",
    "        layers.Dense(n_classes, activation=\"softmax\")  \n",
    "    ]\n",
    ")\n",
    "\n",
    "# View summary\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Reduce lr\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=1e-6)\n",
    "# Train\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32,\n",
    "                    validation_split=0.1, callbacks=[reduce_lr, WandbCallback()])\n",
    "\n",
    "wandb.run.save\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # View plots\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training & validation loss values\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('Model loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View loss\n",
    "\n",
    "# y_test_p = np.argmax(model.predict(X_test), axis=-1)\n",
    "y_test_p = model.predict(X_test)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_p, verbose=1)\n",
    "print(\"Test loss:\", test_loss)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prediction\n",
    "# # y_test_p = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "# print(f\"Classification report:\\n\"\n",
    "#       f\"{classification_report(y_test, y_test_p)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './best_model.h5'\n",
    "model.save(model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('algo_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fffb08dfde95af5737aaafa8384c56a1363405faa584e5dcc194583aeafbe44b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
