{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "\n",
    "import sklearn.preprocessing as preproc\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, plot_confusion_matrix, ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense, Dropout, MaxPooling1D\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import layers\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from gc import callbacks\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('./data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>font_size</th>\n",
       "      <th>fontname</th>\n",
       "      <th>text</th>\n",
       "      <th>has_fullstop</th>\n",
       "      <th>text_length</th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>y0</th>\n",
       "      <th>y1</th>\n",
       "      <th>height</th>\n",
       "      <th>size_diff_prev</th>\n",
       "      <th>xdiff</th>\n",
       "      <th>ydiff</th>\n",
       "      <th>isHeader</th>\n",
       "      <th>headerLevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>Symmetries in quantum field theory\\n</td>\n",
       "      <td>Symmetries in quantum field theory</td>\n",
       "      <td>False</td>\n",
       "      <td>35</td>\n",
       "      <td>181.252</td>\n",
       "      <td>429.988707</td>\n",
       "      <td>612.034997</td>\n",
       "      <td>629.250397</td>\n",
       "      <td>17.215400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>Jiwoo Park\\n</td>\n",
       "      <td>Jiwoo Park</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>276.849</td>\n",
       "      <td>334.398942</td>\n",
       "      <td>584.181691</td>\n",
       "      <td>596.136891</td>\n",
       "      <td>11.955200</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>9.559700e+01</td>\n",
       "      <td>-5.260200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>November 2021\\n</td>\n",
       "      <td>November 2021</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>266.281</td>\n",
       "      <td>344.970126</td>\n",
       "      <td>560.735691</td>\n",
       "      <td>572.690891</td>\n",
       "      <td>11.955200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.056800e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>1 Noether’s theorem\\n</td>\n",
       "      <td>1 Noether’s theorem</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>133.768</td>\n",
       "      <td>289.985206</td>\n",
       "      <td>517.363837</td>\n",
       "      <td>531.710037</td>\n",
       "      <td>14.346200</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.325130e+02</td>\n",
       "      <td>2.391000</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>→\\n</td>\n",
       "      <td>Classical Noether’s theorem. Say that we have ...</td>\n",
       "      <td>True</td>\n",
       "      <td>140</td>\n",
       "      <td>133.768</td>\n",
       "      <td>477.479600</td>\n",
       "      <td>484.438256</td>\n",
       "      <td>506.355856</td>\n",
       "      <td>21.917600</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-2.842171e-14</td>\n",
       "      <td>7.571400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>10</td>\n",
       "      <td>ASYTPY+CMBX10</td>\n",
       "      <td>[48] J. Polchinski, Eﬀective ﬁeld theory and t...</td>\n",
       "      <td>True</td>\n",
       "      <td>230</td>\n",
       "      <td>111.969</td>\n",
       "      <td>483.306504</td>\n",
       "      <td>253.996350</td>\n",
       "      <td>288.427856</td>\n",
       "      <td>34.431506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.047900e+01</td>\n",
       "      <td>24.468906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3039</th>\n",
       "      <td>10</td>\n",
       "      <td>KFNGAX+CMR10</td>\n",
       "      <td>(1996) [hep-ph/9512380].</td>\n",
       "      <td>True</td>\n",
       "      <td>25</td>\n",
       "      <td>132.448</td>\n",
       "      <td>241.481683</td>\n",
       "      <td>242.041350</td>\n",
       "      <td>252.003950</td>\n",
       "      <td>9.962600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.047900e+01</td>\n",
       "      <td>-24.468906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040</th>\n",
       "      <td>10</td>\n",
       "      <td>EYSKRI+CMTI10</td>\n",
       "      <td>[51] J. Wess and J. Bagger, Supersymmetry and ...</td>\n",
       "      <td>True</td>\n",
       "      <td>81</td>\n",
       "      <td>111.969</td>\n",
       "      <td>483.310499</td>\n",
       "      <td>230.086350</td>\n",
       "      <td>240.606856</td>\n",
       "      <td>10.520506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.047900e+01</td>\n",
       "      <td>0.557906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3041</th>\n",
       "      <td>10</td>\n",
       "      <td>KFNGAX+CMR10</td>\n",
       "      <td>Press, USA, 1992).</td>\n",
       "      <td>True</td>\n",
       "      <td>19</td>\n",
       "      <td>132.448</td>\n",
       "      <td>214.643435</td>\n",
       "      <td>218.131350</td>\n",
       "      <td>228.093950</td>\n",
       "      <td>9.962600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.047900e+01</td>\n",
       "      <td>-0.557906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>10</td>\n",
       "      <td>ASYTPY+CMBX10</td>\n",
       "      <td>[52] L. F. Abbott, Nucl. Phys. B 185, 189 (198...</td>\n",
       "      <td>True</td>\n",
       "      <td>201</td>\n",
       "      <td>111.969</td>\n",
       "      <td>336.851585</td>\n",
       "      <td>170.310350</td>\n",
       "      <td>216.696856</td>\n",
       "      <td>46.386506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.047900e+01</td>\n",
       "      <td>36.423906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3043 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      font_size                              fontname  \\\n",
       "0            17  Symmetries in quantum field theory\\n   \n",
       "1            12                          Jiwoo Park\\n   \n",
       "2            12                       November 2021\\n   \n",
       "3            14                 1 Noether’s theorem\\n   \n",
       "4            10                                   →\\n   \n",
       "...         ...                                   ...   \n",
       "3038         10                         ASYTPY+CMBX10   \n",
       "3039         10                          KFNGAX+CMR10   \n",
       "3040         10                         EYSKRI+CMTI10   \n",
       "3041         10                          KFNGAX+CMR10   \n",
       "3042         10                         ASYTPY+CMBX10   \n",
       "\n",
       "                                                   text  has_fullstop  \\\n",
       "0                   Symmetries in quantum field theory          False   \n",
       "1                                           Jiwoo Park          False   \n",
       "2                                        November 2021          False   \n",
       "3                                  1 Noether’s theorem          False   \n",
       "4     Classical Noether’s theorem. Say that we have ...          True   \n",
       "...                                                 ...           ...   \n",
       "3038  [48] J. Polchinski, Eﬀective ﬁeld theory and t...          True   \n",
       "3039                          (1996) [hep-ph/9512380].           True   \n",
       "3040  [51] J. Wess and J. Bagger, Supersymmetry and ...          True   \n",
       "3041                                Press, USA, 1992).           True   \n",
       "3042  [52] L. F. Abbott, Nucl. Phys. B 185, 189 (198...          True   \n",
       "\n",
       "      text_length       x0          x1          y0          y1     height  \\\n",
       "0              35  181.252  429.988707  612.034997  629.250397  17.215400   \n",
       "1              11  276.849  334.398942  584.181691  596.136891  11.955200   \n",
       "2              14  266.281  344.970126  560.735691  572.690891  11.955200   \n",
       "3              20  133.768  289.985206  517.363837  531.710037  14.346200   \n",
       "4             140  133.768  477.479600  484.438256  506.355856  21.917600   \n",
       "...           ...      ...         ...         ...         ...        ...   \n",
       "3038          230  111.969  483.306504  253.996350  288.427856  34.431506   \n",
       "3039           25  132.448  241.481683  242.041350  252.003950   9.962600   \n",
       "3040           81  111.969  483.310499  230.086350  240.606856  10.520506   \n",
       "3041           19  132.448  214.643435  218.131350  228.093950   9.962600   \n",
       "3042          201  111.969  336.851585  170.310350  216.696856  46.386506   \n",
       "\n",
       "      size_diff_prev         xdiff      ydiff isHeader  headerLevel  \n",
       "0                NaN           NaN        NaN     True          1.0  \n",
       "1               -5.0  9.559700e+01  -5.260200      NaN          NaN  \n",
       "2                0.0 -1.056800e+01   0.000000      NaN          NaN  \n",
       "3                2.0 -1.325130e+02   2.391000     True          2.0  \n",
       "4               -4.0 -2.842171e-14   7.571400      NaN          NaN  \n",
       "...              ...           ...        ...      ...          ...  \n",
       "3038             0.0 -2.047900e+01  24.468906      NaN          NaN  \n",
       "3039             0.0  2.047900e+01 -24.468906      NaN          NaN  \n",
       "3040             0.0 -2.047900e+01   0.557906      NaN          NaN  \n",
       "3041             0.0  2.047900e+01  -0.557906      NaN          NaN  \n",
       "3042             0.0 -2.047900e+01  36.423906      NaN          NaN  \n",
       "\n",
       "[3043 rows x 15 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Text preprocessing/cleaning function'''\n",
    "    \n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove symbols, special characters\n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Tokenize \n",
    "    text =  nltk.WordPunctTokenizer().tokenize(text)\n",
    "        \n",
    "    return text\n",
    "data_df['Cleaned'] = list(map(clean_text, data_df.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_df['Cleaned']\n",
    "y_train = data_df[['isHeader', 'headerLevel']].replace(np.nan,-1).replace(True,1)\n",
    "y_train['isHeader'] = y_train['isHeader'].replace(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     Symmetries in quantum field theory \n",
       "1                                             Jiwoo Park \n",
       "2                                          November 2021 \n",
       "3                                    1 Noether’s theorem \n",
       "4       Classical Noether’s theorem. Say that we have ...\n",
       "                              ...                        \n",
       "3038    [48] J. Polchinski, Eﬀective ﬁeld theory and t...\n",
       "3039                            (1996) [hep-ph/9512380]. \n",
       "3040    [51] J. Wess and J. Bagger, Supersymmetry and ...\n",
       "3041                                  Press, USA, 1992). \n",
       "3042    [52] L. F. Abbott, Nucl. Phys. B 185, 189 (198...\n",
       "Name: text, Length: 3043, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isHeader</th>\n",
       "      <th>headerLevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3039</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3041</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3043 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      isHeader  headerLevel\n",
       "0            1          1.0\n",
       "1            0         -1.0\n",
       "2            0         -1.0\n",
       "3            1          2.0\n",
       "4            0         -1.0\n",
       "...        ...          ...\n",
       "3038         0         -1.0\n",
       "3039         0         -1.0\n",
       "3040         0         -1.0\n",
       "3041         0         -1.0\n",
       "3042         0         -1.0\n",
       "\n",
       "[3043 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                   vector_size=256,\n",
    "                                   window=4,\n",
    "                                   min_count=1)\n",
    "w2v_model.wv['lstm'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79104/1965191243.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train_vec = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n"
     ]
    }
   ],
   "source": [
    "words = set(w2v_model.wv.index_to_key)\n",
    "\n",
    "X_train_vec = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3043,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 256)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: need to pad the word vec's so they have equal length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feaure engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy data to verify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./train.csv')\n",
    "test_df = pd.read_csv('./test.csv')\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Text preprocessing/cleaning function'''\n",
    "    \n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove symbols, special characters\n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Tokenize \n",
    "    text =  nltk.WordPunctTokenizer().tokenize(text)\n",
    "        \n",
    "    return text\n",
    "\n",
    "train_df['Cleaned'] = list(map(clean_text, train_df.text))\n",
    "test_df['Cleaned'] = list(map(clean_text, test_df.text))\n",
    "\n",
    "X_train, y_train = train_df['Cleaned'], train_df['label']\n",
    "X_test, y_test = test_df['Cleaned'], test_df['label']\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                   vector_size=256,\n",
    "                                   window=4,\n",
    "                                   min_count=1)\n",
    "\n",
    "words = set(w2v_model.wv.index_to_key)\n",
    "\n",
    "X_train_vec = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_train])\n",
    "\n",
    "X_test_vec = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_test])\n",
    "\n",
    "X_train = []\n",
    "for v in X_train_vec:\n",
    "    if v.size:\n",
    "        X_train.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_train.append(np.zeros(256, dtype=float))\n",
    "  \n",
    "X_test = []\n",
    "for v in X_test_vec:\n",
    "    if v.size:\n",
    "        X_test.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_test.append(np.zeros(256, dtype=float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['0', '1']\n",
    "\n",
    "# Logistic regression \n",
    "\n",
    "param_log = {'solver':['lbfgs'],\n",
    "              'C':[1, 5, 10, 50, 100]}\n",
    "\n",
    "# Create model\n",
    "logr_clf = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# 10 fold cv hyper-parameters tuning\n",
    "clf_log = GridSearchCV(logr_clf,\n",
    "                       param_grid=param_log, \n",
    "                       cv=10, \n",
    "                       scoring='accuracy',\n",
    "                       refit=True) \n",
    "\n",
    "# Fit\n",
    "clf_log.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_log.best_params_)\n",
    "print(\"Best score\", clf_log.best_score_)\n",
    "\n",
    "# Predict on test\n",
    "logr_y_pred = clf_log.predict(X_test)\n",
    "logr_y_proba = clf_log.predict_proba(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test,logr_y_pred)))\n",
    "print('Classification report\\n', classification_report(y_test, logr_y_pred)) \n",
    "\n",
    "wandb.init(project=\"Title classifier\", reinit=True)\n",
    "wandb.run.name = 'Logistic regression'\n",
    "wandb.sklearn.plot_classifier(clf_log, X_train, X_test, y_train, y_test, logr_y_pred, logr_y_proba, labels,\n",
    "                                                         model_name='Logistic regression', feature_names=None)\n",
    "wandb.run.save\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Svc\n",
    "\n",
    "param_svc = {'kernel':['rbf'],\n",
    "              'C':[0.1, 1, 5, 10],\n",
    "              'gamma': [0.01, 0.1, 1, 5]}\n",
    "\n",
    "# Create model\n",
    "svc_clf = SVC(probability=True)\n",
    "\n",
    "# 10 fold cv hyper-parameters tuning\n",
    "clf_svc = GridSearchCV(svc_clf,\n",
    "                       param_grid=param_svc, \n",
    "                       cv=10, \n",
    "                       scoring='accuracy',\n",
    "                       refit=True) \n",
    "# Fit\n",
    "clf_svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_svc.best_params_)\n",
    "print(\"Best score\", clf_svc.best_score_)\n",
    "\n",
    "# Predict on test\n",
    "svc_y_pred = clf_svc.predict(X_test)\n",
    "svc_y_proba = clf_svc.predict_proba(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test,svc_y_pred)))\n",
    "print('Classification report\\n', classification_report(y_test, svc_y_pred)) \n",
    "\n",
    "wandb.init(project=\"Title classifier\", reinit=True)\n",
    "wandb.run.name = 'Support vector'\n",
    "wandb.sklearn.plot_classifier(clf_svc, X_train, X_test, y_train, y_test, svc_y_pred, svc_y_proba, labels,\n",
    "                                                         model_name='Support vector', feature_names=None)\n",
    "wandb.run.save\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Forest\n",
    "\n",
    "param_rf = { \n",
    "    'n_estimators': [5, 10, 20, 50, 100, 150],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth' : [1,2,3,4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=41)\n",
    "\n",
    "clf_rf = GridSearchCV(estimator=rf_clf, \n",
    "                      param_grid=param_rf, \n",
    "                      scoring='accuracy',\n",
    "                      cv=10,\n",
    "                      refit=True)\n",
    "\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_rf.best_params_)\n",
    "print(\"Best score\", clf_rf.best_score_)\n",
    "\n",
    "rfc_y_pred = clf_rf.predict(X_test)\n",
    "rfc_y_proba = clf_rf.predict_proba(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test, rfc_y_pred)))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, rfc_y_pred))\n",
    "\n",
    "wandb.init(project=\"Title classifier\", reinit=True)\n",
    "wandb.run.name = 'Random forest'\n",
    "wandb.sklearn.plot_classifier(clf_rf, X_train, X_test, y_train, y_test, rfc_y_pred, rfc_y_proba, labels,\n",
    "                                                         model_name='Random forest', feature_names=None)\n",
    "wandb.run.save\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decision tree\n",
    "\n",
    "param_dt = {'max_features': ['sqrt', 'log2'],\n",
    "              'ccp_alpha': [0.5, 0.1, .01, .001],\n",
    "              'max_depth' : [1,2,3,4,5,6,7,8,9],\n",
    "              'criterion' :['gini', 'entropy']\n",
    "             }\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "clf_dt = GridSearchCV(dt_clf, \n",
    "                      param_grid=param_dt, \n",
    "                      scoring='accuracy',\n",
    "                      cv=10,\n",
    "                      refit=True)\n",
    "\n",
    "clf_dt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_dt.best_params_)\n",
    "print(\"Best score\", clf_dt.best_score_)\n",
    "\n",
    "dt_y_pred = clf_dt.predict(X_test)\n",
    "dt_y_proba = clf_dt.predict_proba(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test, dt_y_pred)))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, dt_y_pred))\n",
    "\n",
    "wandb.init(project=\"Title classifier\", reinit=True)\n",
    "wandb.run.name = 'Decision tree'\n",
    "wandb.sklearn.plot_classifier(clf_dt, X_train, X_test, y_train, y_test, dt_y_pred, dt_y_proba, labels,\n",
    "                                                         model_name='Decision tree', feature_names=None)\n",
    "wandb.run.save\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "\n",
    "param_knn = { \n",
    "    'n_neighbors': list(range(1,11)),\n",
    "}\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "clf_knn = GridSearchCV(estimator=rf_clf, \n",
    "                      param_grid=param_rf, \n",
    "                      scoring='accuracy',\n",
    "                      cv=10,\n",
    "                      refit=True)\n",
    "\n",
    "clf_knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_knn.best_params_)\n",
    "print(\"Best score\", clf_knn.best_score_)\n",
    "\n",
    "knn_y_pred = clf_knn.predict(X_test)\n",
    "knn_y_proba = clf_knn.predict_proba(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test, knn_y_pred)))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, knn_y_pred))\n",
    "\n",
    "wandb.init(project=\"Title classifier\", reinit=True)\n",
    "wandb.run.name = 'KNN'\n",
    "wandb.sklearn.plot_classifier(clf_knn, X_train, X_test, y_train, y_test, knn_y_pred, knn_y_proba, labels,\n",
    "                                                         model_name='KNN', feature_names=None)\n",
    "wandb.run.save\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB\n",
    "\n",
    "param_xgb = {\n",
    "        'n_estimators': range(20, 400, 20),\n",
    "        'learning_rate': [1e-1, 1e-2, 5e-3, 5e-4],\n",
    "        'min_child_weight': [1, 3, 5, 7, 9],\n",
    "        'gamma': [0.5, 1, 1.5, 2],\n",
    "        'subsample': [0.5, 0.75, 1.0],\n",
    "        'colsample_bytree': [0.5, 0.75, 1.0],\n",
    "        'max_depth': [2, 3, 4, 5, 6, 7, 8]\n",
    "        }\n",
    "\n",
    "xgb_clf = XGBClassifier(objective='binary:logistic', seed=41)\n",
    "\n",
    "clf_xgb = GridSearchCV(estimator=xgb_clf, \n",
    "                      param_grid=param_xgb, \n",
    "                      scoring='accuracy',\n",
    "                      cv=10,\n",
    "                      refit=True)\n",
    "\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_xgb.best_params_)\n",
    "print(\"Best score\", clf_xgb.best_score_)\n",
    "\n",
    "xgb_y_pred = clf_xgb.predict(X_test)\n",
    "xgb_y_proba = clf_xgb.predict_proba(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test, xgb_y_pred)))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, xgb_y_pred))\n",
    "\n",
    "wandb.init(project=\"Title classifier\", reinit=True)\n",
    "wandb.run.name = 'XGB'\n",
    "wandb.sklearn.plot_classifier(clf_xgb, X_train, X_test, y_train, y_test, xgb_y_pred, xgb_y_proba, labels,\n",
    "                                                         model_name='XGB', feature_names=None)\n",
    "wandb.run.save\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_len = 512\n",
    "max_tokens = 300\n",
    "max_words = 100\n",
    "n_classes = 2\n",
    "\n",
    "sweep_config = {\n",
    "   'method': 'grid',\n",
    "   \n",
    "   'parameters': {\n",
    "       \n",
    "       'neurons': {\n",
    "           'values': [32, 64, 128]\n",
    "       },\n",
    "       \n",
    "       'f': {\n",
    "           'values': [64, 128, 256]\n",
    "       },\n",
    "       \n",
    "       'bs': {\n",
    "           'values': [8, 16, 32, 64]\n",
    "       }\n",
    "   }\n",
    "}\n",
    "\n",
    "wandb.init(project=\"Title classifier\", reinit=True)\n",
    "wandb.run.name = 'CNN model tuning'\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "       \n",
    "   configs = {\n",
    "       'neurons': 32,\n",
    "       'f': 64,\n",
    "       'bs': 8\n",
    "   }\n",
    "   \n",
    "   # Specify the other hyperparameters to the configuration\n",
    "   config = wandb.config\n",
    "   config.epochs = 50\n",
    "   \n",
    "   model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(max_tokens, )),\n",
    "            \n",
    "            layers.Embedding(input_dim=max_words, \n",
    "                             output_dim=embed_len,  \n",
    "                             input_length=max_tokens),\n",
    "            \n",
    "            # Block 1\n",
    "            layers.Conv1D(wandb.config.f, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling1D(pool_size=(3, 3)),\n",
    "            layers.Dropout(0.3),                  \n",
    "            \n",
    "            # Block 2\n",
    "            layers.Conv1D(wandb.config.f, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling1D(pool_size=(3, 3)),\n",
    "            layers.Dropout(0.3),                \n",
    "            \n",
    "            # FC 1\n",
    "            layers.Flatten(),                                     \n",
    "            layers.Dense(wandb.config.neurons, activation=\"relu\"),             \n",
    "            \n",
    "            # Output \n",
    "            layers.Dense(n_classes, activation=\"softmax\")     \n",
    "        ]\n",
    "    )\n",
    "\n",
    "   model.compile(optimizer='adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "   \n",
    "   model.fit(X_train, y_train, epochs=config.epochs, batch_size=config.bs,\n",
    "             validation_split=0.1, callbacks=[WandbCallback()])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.save\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create model\n",
    "\n",
    "# embed_len = 512\n",
    "# max_tokens = 300\n",
    "# max_words = 100\n",
    "# n_classes = 2\n",
    "\n",
    "# def create_model(f, neuron):\n",
    "    \n",
    "#     model = keras.Sequential(\n",
    "#         [\n",
    "#             keras.Input(shape=(max_tokens, )),\n",
    "            \n",
    "#             layers.Embedding(input_dim=max_words, \n",
    "#                              output_dim=embed_len,  \n",
    "#                              input_length=max_tokens),\n",
    "            \n",
    "#             # Block 1\n",
    "#             layers.Conv1D(f, kernel_size=(3, 3), activation=\"relu\"),\n",
    "#             layers.MaxPooling1D(pool_size=(3, 3)),\n",
    "#             layers.Dropout(0.3),                  \n",
    "            \n",
    "#             # Block 2\n",
    "#             layers.Conv1D(f, kernel_size=(3, 3), activation=\"relu\"),\n",
    "#             layers.MaxPooling1D(pool_size=(3, 3)),\n",
    "#             layers.Dropout(0.3),                \n",
    "            \n",
    "#             # FC 1\n",
    "#             layers.Flatten(),                                     \n",
    "#             layers.Dense(neuron, activation=\"relu\"),             \n",
    "            \n",
    "#             # Output \n",
    "#             layers.Dense(n_classes, activation=\"softmax\")     \n",
    "#         ]\n",
    "#     )\n",
    "    \n",
    "#     model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KerasClassifier(build_fn=create_model,\n",
    "#                         f=[128,256],\n",
    "#                         epochs=[50],\n",
    "#                         neuron=[32,64],\n",
    "#                         batch_size=[32,64],)\n",
    "\n",
    "# # Model params\n",
    "# params={'batch_size':[32,64], \n",
    "#         'epochs':[50],\n",
    "#         'neuron':[32,64],\n",
    "#         'f':[128,256],\n",
    "#         }\n",
    "\n",
    "# # Grid search cv\n",
    "# gs = GridSearchCV(estimator=model, \n",
    "#                   param_grid=params, \n",
    "#                   cv=10,\n",
    "#                   refit=True)\n",
    "# # Fit\n",
    "# gs = gs.fit(X_train, y_train)\n",
    "\n",
    "# print('Best acc:', gs.best_score_)\n",
    "# print('Best params:', gs.best_params_)\n",
    "\n",
    "# mean = gs.cv_results_['mean_test_score']\n",
    "# stds = gs.cv_results_['std_test_score']\n",
    "# params = gs.cv_results_['params']\n",
    "\n",
    "# for mean, stdev, param in zip(mean, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain with best parameters\n",
    "\n",
    "wandb.init(project=\"Title classifier\", reinit=True)\n",
    "wandb.run.name = 'Best params CNN model training'\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(max_tokens, )),\n",
    "        \n",
    "        layers.Embedding(input_dim=max_words, \n",
    "                    output_dim=embed_len,  \n",
    "                    input_length=max_tokens),\n",
    "        \n",
    "        # Block 1\n",
    "        layers.Conv1D(256, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling1D(pool_size=(3, 3)),                  \n",
    "        layers.Dropout(0.3),                  \n",
    "\n",
    "        # Block 2\n",
    "        layers.Conv1D(256, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling1D(pool_size=(3, 3)),                \n",
    "        layers.Dropout(0.3),                  \n",
    "\n",
    "        # FC 1\n",
    "        layers.Flatten(),                                     \n",
    "        layers.Dense(64, activation=\"relu\"),                \n",
    "        \n",
    "        # Output \n",
    "        layers.Dense(n_classes, activation=\"softmax\")  \n",
    "    ]\n",
    ")\n",
    "\n",
    "# View summary\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Reduce lr\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=1e-6)\n",
    "# Train\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32,\n",
    "                    validation_split=0.1, callbacks=[reduce_lr, WandbCallback()])\n",
    "\n",
    "wandb.run.save\n",
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # View plots\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training & validation loss values\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('Model loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View loss\n",
    "\n",
    "# y_test_p = np.argmax(model.predict(X_test), axis=-1)\n",
    "y_test_p = model.predict(X_test)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_p, verbose=1)\n",
    "print(\"Test loss:\", test_loss)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prediction\n",
    "# # y_test_p = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "# print(f\"Classification report:\\n\"\n",
    "#       f\"{classification_report(y_test, y_test_p)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './best_model.h5'\n",
    "model.save(model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "fffb08dfde95af5737aaafa8384c56a1363405faa584e5dcc194583aeafbe44b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
