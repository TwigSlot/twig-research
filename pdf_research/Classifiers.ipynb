{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing as preproc\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, plot_confusion_matrix, ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, Dense, Dropout, MaxPooling1D\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import layers\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from gc import callbacks\n",
    "\n",
    "import pickle\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feaure engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(project=\"Title classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logistic regression \n",
    "\n",
    "param_log = {'solver':['lbfgs'],\n",
    "              'C':[1, 5, 10, 50, 100],\n",
    "              'penalty': ['12']}\n",
    "\n",
    "# Create model\n",
    "logr_clf = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# 10 fold cv hyper-parameters tuning\n",
    "clf_log = GridSearchCV(logr_clf,\n",
    "                       param_grid=param_log, \n",
    "                       cv=10, \n",
    "                       scoring='accuracy',\n",
    "                       refit=True) \n",
    "\n",
    "# Fit\n",
    "clf_log.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_log.best_params_)\n",
    "print(\"Best score\", clf_log.best_score_)\n",
    "\n",
    "# Predict on test\n",
    "logr_y_pred = clf_log.predict(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test,logr_y_pred)))\n",
    "print('Classification report\\n', classification_report(y_test, logr_y_pred)) \n",
    "\n",
    "wandb.sklearn.plot_classifier(clf_log, X_train, X_test, y_train, y_test, logr_y_pred,\n",
    "                                                         model_name='Logistic regression', feature_names=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Svc\n",
    "\n",
    "param_svc = {'kernel':['rbf'],\n",
    "              'C':[0.1, 1, 5, 10, 50, 100],\n",
    "              'gamma': [0.01, 0.1, 1, 5, 10, 50, 100]}\n",
    "\n",
    "# Create model\n",
    "svc_clf = SVC()\n",
    "\n",
    "# 10 fold cv hyper-parameters tuning\n",
    "clf_svc = GridSearchCV(svc_clf,\n",
    "                       param_grid=param_svc, \n",
    "                       cv=10, \n",
    "                       scoring='accuracy',\n",
    "                       refit=True) \n",
    "# Fit\n",
    "clf_svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_svc.best_params_)\n",
    "print(\"Best score\", clf_svc.best_score_)\n",
    "\n",
    "# Predict on test\n",
    "svc_y_pred = clf_svc.predict(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test,svc_y_pred)))\n",
    "print('Classification report\\n', classification_report(y_test, svc_y_pred)) \n",
    "\n",
    "wandb.sklearn.plot_classifier(clf_svc, X_train, X_test, y_train, y_test, svc_y_pred,\n",
    "                                                         model_name='Support vector', feature_names=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Forest\n",
    "\n",
    "param_rf = { \n",
    "    'n_estimators': [5, 10, 20, 50, 100, 150, 200],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth' : [1,2,3,4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=41)\n",
    "\n",
    "clf_rf = GridSearchCV(estimator=rf_clf, \n",
    "                      param_grid=param_rf, \n",
    "                      scoring='accuracy',\n",
    "                      cv=10,\n",
    "                      refit=True)\n",
    "\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_rf.best_params_)\n",
    "print(\"Best score\", clf_rf.best_score_)\n",
    "\n",
    "rfc_y_pred = clf_rf.predict(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test, rfc_y_pred)))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, rfc_y_pred))\n",
    "\n",
    "wandb.sklearn.plot_classifier(clf_rf, X_train, X_test, y_train, y_test, rfc_y_pred,\n",
    "                                                         model_name='Random forest', feature_names=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decision tree\n",
    "\n",
    "param_dt = {'max_features': ['sqrt', 'log2'],\n",
    "              'ccp_alpha': [0.5, 0.1, .01, .001],\n",
    "              'max_depth' : [1,2,3,4,5,6,7,8,9],\n",
    "              'criterion' :['gini', 'entropy']\n",
    "             }\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "\n",
    "clf_dt = GridSearchCV(dt_clf, \n",
    "                      param_grid=param_dt, \n",
    "                      scoring='accuracy',\n",
    "                      cv=10,\n",
    "                      refit=True)\n",
    "\n",
    "clf_dt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_dt.best_params_)\n",
    "print(\"Best score\", clf_dt.best_score_)\n",
    "\n",
    "dt_y_pred = clf_dt.predict(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test, dt_y_pred)))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, dt_y_pred))\n",
    "\n",
    "wandb.sklearn.plot_classifier(clf_dt, X_train, X_test, y_train, y_test, dt_y_pred,\n",
    "                                                         model_name='Decision tree', feature_names=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "\n",
    "param_knn = { \n",
    "    'n_neighbors': list(range(1,11)),\n",
    "}\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "clf_knn = GridSearchCV(estimator=rf_clf, \n",
    "                      param_grid=param_rf, \n",
    "                      scoring='accuracy',\n",
    "                      cv=10,\n",
    "                      refit=True)\n",
    "\n",
    "clf_knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_knn.best_params_)\n",
    "print(\"Best score\", clf_knn.best_score_)\n",
    "\n",
    "knn_y_pred = clf_knn.predict(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test, knn_y_pred)))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, knn_y_pred))\n",
    "\n",
    "wandb.sklearn.plot_classifier(clf_knn, X_train, X_test, y_train, y_test, knn_y_pred,\n",
    "                                                         model_name='KNN', feature_names=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB\n",
    "\n",
    "param_xgb = {\n",
    "        'n_estimators': range(20, 400, 20),\n",
    "        'learning_rate': [1e-1, 1e-2, 5e-3, 5e-4],\n",
    "        'min_child_weight': [1, 3, 5, 7, 9],\n",
    "        'gamma': [0.5, 1, 1.5, 2],\n",
    "        'subsample': [0.5, 0.75, 1.0],\n",
    "        'colsample_bytree': [0.5, 0.75, 1.0],\n",
    "        'max_depth': [2, 3, 4, 5, 6, 7, 8]\n",
    "        }\n",
    "\n",
    "xgb_clf = XGBClassifier(objective='binary:logistic', seed=41)\n",
    "\n",
    "clf_xgb = GridSearchCV(estimator=xgb_clf, \n",
    "                      param_grid=param_xgb, \n",
    "                      scoring='accuracy',\n",
    "                      cv=10,\n",
    "                      refit=True)\n",
    "\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters\", clf_xgb.best_params_)\n",
    "print(\"Best score\", clf_xgb.best_score_)\n",
    "\n",
    "xgb_y_pred = clf_xgb.predict(X_test)\n",
    "\n",
    "print('Accuracy Score: ' + str(accuracy_score(y_test, xgb_y_pred)))\n",
    "print(\"Classification report: \\n\", classification_report(y_test, xgb_y_pred))\n",
    "\n",
    "wandb.sklearn.plot_classifier(clf_xgb, X_train, X_test, y_train, y_test, xgb_y_pred,\n",
    "                                                         model_name='XGB', feature_names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_len = 512\n",
    "max_tokens = 300\n",
    "max_words = 100\n",
    "n_classes = 2\n",
    "\n",
    "sweep_config = {\n",
    "   'method': 'grid',\n",
    "   \n",
    "   'parameters': {\n",
    "       \n",
    "       'neurons': {\n",
    "           'values': [32, 64, 128]\n",
    "       },\n",
    "       \n",
    "       'f': {\n",
    "           'values': [64, 128, 256]\n",
    "       },\n",
    "       \n",
    "       'bs': {\n",
    "           'values': [8, 16, 32, 64]\n",
    "       }\n",
    "   }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "       \n",
    "   configs = {\n",
    "       'neurons': 32,\n",
    "       'f': 64,\n",
    "       'bs': 8\n",
    "   }\n",
    "   \n",
    "   # Specify the other hyperparameters to the configuration\n",
    "   config = wandb.config\n",
    "   config.epochs = 50\n",
    "   \n",
    "   model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(max_tokens, )),\n",
    "            \n",
    "            layers.Embedding(input_dim=max_words, \n",
    "                             output_dim=embed_len,  \n",
    "                             input_length=max_tokens),\n",
    "            \n",
    "            # Block 1\n",
    "            layers.Conv1D(wandb.config.f, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling1D(pool_size=(3, 3)),\n",
    "            layers.Dropout(0.3),                  \n",
    "            \n",
    "            # Block 2\n",
    "            layers.Conv1D(wandb.config.f, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling1D(pool_size=(3, 3)),\n",
    "            layers.Dropout(0.3),                \n",
    "            \n",
    "            # FC 1\n",
    "            layers.Flatten(),                                     \n",
    "            layers.Dense(wandb.config.neurons, activation=\"relu\"),             \n",
    "            \n",
    "            # Output \n",
    "            layers.Dense(n_classes, activation=\"softmax\")     \n",
    "        ]\n",
    "    )\n",
    "\n",
    "   model.compile(optimizer='adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "   \n",
    "   model.fit(X_train, y_train, epochs=config.epochs, batch_size=config.bs,\n",
    "             validation_split=0.1, callbacks=[WandbCallback()])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, function=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create model\n",
    "\n",
    "# embed_len = 512\n",
    "# max_tokens = 300\n",
    "# max_words = 100\n",
    "# n_classes = 2\n",
    "\n",
    "# def create_model(f, neuron):\n",
    "    \n",
    "#     model = keras.Sequential(\n",
    "#         [\n",
    "#             keras.Input(shape=(max_tokens, )),\n",
    "            \n",
    "#             layers.Embedding(input_dim=max_words, \n",
    "#                              output_dim=embed_len,  \n",
    "#                              input_length=max_tokens),\n",
    "            \n",
    "#             # Block 1\n",
    "#             layers.Conv1D(f, kernel_size=(3, 3), activation=\"relu\"),\n",
    "#             layers.MaxPooling1D(pool_size=(3, 3)),\n",
    "#             layers.Dropout(0.3),                  \n",
    "            \n",
    "#             # Block 2\n",
    "#             layers.Conv1D(f, kernel_size=(3, 3), activation=\"relu\"),\n",
    "#             layers.MaxPooling1D(pool_size=(3, 3)),\n",
    "#             layers.Dropout(0.3),                \n",
    "            \n",
    "#             # FC 1\n",
    "#             layers.Flatten(),                                     \n",
    "#             layers.Dense(neuron, activation=\"relu\"),             \n",
    "            \n",
    "#             # Output \n",
    "#             layers.Dense(n_classes, activation=\"softmax\")     \n",
    "#         ]\n",
    "#     )\n",
    "    \n",
    "#     model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KerasClassifier(build_fn=create_model,\n",
    "#                         f=[128,256],\n",
    "#                         epochs=[50],\n",
    "#                         neuron=[32,64],\n",
    "#                         batch_size=[32,64],)\n",
    "\n",
    "# # Model params\n",
    "# params={'batch_size':[32,64], \n",
    "#         'epochs':[50],\n",
    "#         'neuron':[32,64],\n",
    "#         'f':[128,256],\n",
    "#         }\n",
    "\n",
    "# # Grid search cv\n",
    "# gs = GridSearchCV(estimator=model, \n",
    "#                   param_grid=params, \n",
    "#                   cv=10,\n",
    "#                   refit=True)\n",
    "# # Fit\n",
    "# gs = gs.fit(X_train, y_train)\n",
    "\n",
    "# print('Best acc:', gs.best_score_)\n",
    "# print('Best params:', gs.best_params_)\n",
    "\n",
    "# mean = gs.cv_results_['mean_test_score']\n",
    "# stds = gs.cv_results_['std_test_score']\n",
    "# params = gs.cv_results_['params']\n",
    "\n",
    "# for mean, stdev, param in zip(mean, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain with best parameters\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(max_tokens, )),\n",
    "        \n",
    "        layers.Embedding(input_dim=max_words, \n",
    "                    output_dim=embed_len,  \n",
    "                    input_length=max_tokens),\n",
    "        \n",
    "        # Block 1\n",
    "        layers.Conv1D(256, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling1D(pool_size=(3, 3)),                  \n",
    "        layers.Dropout(0.3),                  \n",
    "\n",
    "        # Block 2\n",
    "        layers.Conv1D(256, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling1D(pool_size=(3, 3)),                \n",
    "        layers.Dropout(0.3),                  \n",
    "\n",
    "        # FC 1\n",
    "        layers.Flatten(),                                     \n",
    "        layers.Dense(64, activation=\"relu\"),                \n",
    "        \n",
    "        # Output \n",
    "        layers.Dense(n_classes, activation=\"softmax\")  \n",
    "    ]\n",
    ")\n",
    "\n",
    "# View summary\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Reduce lr\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=1e-6)\n",
    "# Train\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32,\n",
    "                    validation_split=0.1, callbacks=[reduce_lr, WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # View plots\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('Model accuracy')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training & validation loss values\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('Model loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View loss\n",
    "\n",
    "# y_test_p = np.argmax(model.predict(X_test), axis=-1)\n",
    "y_test_p = model.predict(X_test)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_p, verbose=1)\n",
    "print(\"Test loss:\", test_loss)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prediction\n",
    "# # y_test_p = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "# print(f\"Classification report:\\n\"\n",
    "#       f\"{classification_report(y_test, y_test_p)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './best_model.h5'\n",
    "model.save(model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ba9a036c33cd0aea3e2c62600d783e80b2e342a21630d56ef8593f6c4249295"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
